import logging
import os
from pathlib import Path
from llama_cpp import Llama

logger = logging.getLogger("ark_llm")

# --- –°–ò–°–¢–ï–ú–ù–ò–ô –ü–†–û–ú–ü–¢ (V4: UNIVERSAL ANALYST) ---
# –¶–µ–π –ø—Ä–æ–º–ø—Ç –ø—Ä–∞—Ü—é—î –∑ –±—É–¥—å-—è–∫–∏–º–∏ –¥–∞–Ω–∏–º–∏: –≤—ñ–¥ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ–π –¥–æ —Ö—É–¥–æ–∂–Ω—ñ—Ö —Ç–µ–∫—Å—Ç—ñ–≤.
SYSTEM_PROMPT_TEXT = """–¢–ò ‚Äî –£–ù–Ü–í–ï–†–°–ê–õ–¨–ù–ò–ô –ê–ù–ê–õ–Ü–¢–ò–ß–ù–ò–ô –ê–°–ò–°–¢–ï–ù–¢.
–¢–í–û–Ø –¶–Ü–õ–¨: –ù–∞–¥–∞—Ç–∏ –≤–∏—á–µ—Ä–ø–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å, —Å–ø–∏—Ä–∞—é—á–∏—Å—å –≤–∏–∫–ª—é—á–Ω–æ –Ω–∞ –±–ª–æ–∫ "–ö–û–ù–¢–ï–ö–°–¢".

–ü–†–ê–í–ò–õ–ê –†–û–ë–û–¢–ò:
1. –ü–†–Ü–û–†–ò–¢–ï–¢ –ö–û–ù–¢–ï–ö–°–¢–£: –Ü–≥–Ω–æ—Ä—É–π —Å–≤–æ—ó –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞–Ω–Ω—è. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ç—ñ–ª—å–∫–∏ —Ç–µ, —â–æ –Ω–∞–ø–∏—Å–∞–Ω–æ –Ω–∏–∂—á–µ.
2. –†–û–ó–£–ú–Ü–ù–ù–Ø –°–£–¢–Ü: –Ø–∫—â–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø–∏—Ç–∞—î –∑–∞–≥–∞–ª—å–Ω–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "—è–∫–∞ —Ñ–æ—Ä–º—É–ª–∞", "—è–∫ –æ—Ü—ñ–Ω—é—é—Ç—å", "–ø—Ä–∞–≤–∏–ª–∞"), –∞ –≤ —Ç–µ–∫—Å—Ç—ñ —Ü–µ –Ω–∞–∑–∏–≤–∞—î—Ç—å—Å—è —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–æ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "–Ü–Ω–¥–µ–∫—Å X", "–ú–µ—Ç—Ä–∏–∫–∞ Y", "–ü—Ä–æ—Ç–æ–∫–æ–ª –±–µ–∑–ø–µ–∫–∏") ‚Äî —Ç–∏ –ü–û–í–ò–ù–ï–ù —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ —Ü–µ —ñ –Ω–∞–≤–µ—Å—Ç–∏ —è–∫ –≤—ñ–¥–ø–æ–≤—ñ–¥—å.
3. –¢–û–ß–ù–Ü–°–¢–¨: –¶–∏—Ñ—Ä–∏, —Ñ–æ—Ä–º—É–ª–∏, —ñ–º–µ–Ω–∞ —Ç–∞ –Ω–∞–∑–≤–∏ –ø–µ—Ä–µ–¥–∞–≤–∞–π –±–µ–∑ –∑–º—ñ–Ω.
4. –í–Ü–î–°–£–¢–ù–Ü–°–¢–¨ –§–ê–ù–¢–ê–ó–Ü–ô: –ù–µ –≤–∏–≥–∞–¥—É–π –ø—Ä–∏–∫–ª–∞–¥–∏ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—ñ–≤, —è–∫—â–æ —ó—Ö –Ω–µ–º–∞—î –≤ —Ç–µ–∫—Å—Ç—ñ.
5. –ß–ï–°–ù–Ü–°–¢–¨: –Ø–∫—â–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –Ω–µ–º–∞—î –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ, –Ω–∞–ø–∏—à–∏: "–£ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –≤—ñ–¥—Å—É—Ç–Ω—è".
"""

class LLMEngine:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(LLMEngine, cls).__new__(cls)
            cls._instance._model = None
        return cls._instance

    def load_model(self):
        if self._model is not None: return

        default_path = Path("/app/models_cache/qwen2.5-3b-instruct.gguf")
        if default_path.exists():
            model_path = default_path
        else:
            found = list(Path("/app/models_cache").glob("*.gguf"))
            if found: model_path = found[0]
            else: raise FileNotFoundError("Model not found!")

        logger.info(f"üöÄ Loading LLM: {model_path}")
        
        # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –ø—ñ–¥ i3-1115G4
        total_threads = os.cpu_count() or 4
        safe_threads = max(1, total_threads - 1)

        self._model = Llama(
            model_path=str(model_path),
            n_ctx=4096,           
            n_threads=safe_threads, 
            n_batch=512,          # –ë–∞–ª–∞–Ω—Å —à–≤–∏–¥–∫–æ—Å—Ç—ñ —Ç–∞ —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
            use_mmap=True,        
            use_mlock=False,      
            verbose=False
        )

    def generate_stream(self, query: str, context: str):
        self.load_model()
        
        # –û–±—Ä—ñ–∑–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ (–±–µ–∑–ø–µ—á–Ω–∏–π –ª—ñ–º—ñ—Ç)
        limit = 3000
        if len(context) > limit:
            context = context[:limit] + "..."

        messages = [
            {"role": "system", "content": SYSTEM_PROMPT_TEXT},
            {"role": "user", "content": f"–ö–û–ù–¢–ï–ö–°–¢:\n{context}\n\n–ó–ê–ü–ò–¢–ê–ù–ù–Ø:\n{query}"}
        ]

        stream = self._model.create_chat_completion(
            messages=messages,
            max_tokens=1024,
            # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ 0.2 —ñ–¥–µ–∞–ª—å–Ω–∞ –¥–ª—è —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏—Ö –∑–∞–¥–∞—á:
            # –í–æ–Ω–∞ –¥–æ–∑–≤–æ–ª—è—î –∑—Ä–æ–∑—É–º—ñ—Ç–∏, —â–æ "–æ—Ü—ñ–Ω–∫–∞" = "–º—É—Ä–∞—à–∫–æ–≤–∏–π —ñ–Ω–¥–µ–∫—Å",
            # –∞–ª–µ –Ω–µ –¥–æ–∑–≤–æ–ª—è—î –≤–∏–≥–∞–¥—É–≤–∞—Ç–∏ –Ω–µ—ñ—Å–Ω—É—é—á—ñ —Ñ–∞–∫—Ç–∏.
            temperature=0.2, 
            stream=True
        )

        for chunk in stream:
            delta = chunk["choices"][0]["delta"]
            if "content" in delta:
                yield delta["content"]
