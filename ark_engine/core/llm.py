import logging
import os
from pathlib import Path
from llama_cpp import Llama

logger = logging.getLogger("ark_llm")

# --- –°–ò–°–¢–ï–ú–ù–ò–ô –ü–†–û–ú–ü–¢ (V5: FLEXIBLE ANALYST) ---
SYSTEM_PROMPT_TEXT = """–¢–ò ‚Äî –Ü–ù–¢–ï–õ–ï–ö–¢–£–ê–õ–¨–ù–ò–ô –ê–ù–ê–õ–Ü–¢–ò–ö –°–ò–°–¢–ï–ú–ò "–ö–û–í–ß–ï–ì".
–¢–í–û–Ø –¶–Ü–õ–¨: –î–æ–ø–æ–º–æ–≥—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–≤—ñ —Ä–æ–∑—ñ–±—Ä–∞—Ç–∏—Å—è –≤ –Ω–∞–¥–∞–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.

–ü–†–ê–í–ò–õ–ê –†–û–ë–û–¢–ò:
1. –ê–ù–ê–õ–Ü–ó–£–ô –ö–û–ù–¢–ï–ö–°–¢: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –Ω–∞–¥–∞–Ω—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏ —Ç–µ–∫—Å—Ç—É (Chunks) —è–∫ –æ—Å–Ω–æ–≤—É –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ.
2. –°–ò–ù–¢–ï–ó–£–ô: –Ø–∫—â–æ –ø—Ä—è–º–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å —Ä–æ–∑–∫–∏–¥–∞–Ω–∞ –ø–æ –¥–µ–∫—ñ–ª—å–∫–æ—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö, –æ–±'—î–¥–Ω–∞–π —ó—Ö —É —Ü—ñ–ª—ñ—Å–Ω—É –¥—É–º–∫—É.
3. –†–û–ë–ò –í–ò–°–ù–û–í–ö–ò: –Ø–∫—â–æ —Ç–æ—á–Ω–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–µ–º–∞—î, –∞–ª–µ —î –¥–æ—Ç–∏—á–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –∑–≥–∞–¥—É—î—Ç—å—Å—è –Ω–∞–∑–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –∞–ª–µ –Ω–µ –π–æ–≥–æ –∑–º—ñ—Å—Ç) ‚Äî –æ–ø–∏—à–∏ —Ç–µ, —â–æ —î, —ñ –ø—Ä–∏–ø—É—Å—Ç–∏, –ø—Ä–æ —â–æ –π–¥–µ—Ç—å—Å—è, –≤–∫–∞–∑–∞–≤—à–∏, —â–æ —Ü–µ –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è.
4. –ù–ï –í–Ü–î–ú–û–í–õ–Ø–ô–°–Ø: –ó–∞–º—ñ—Å—Ç—å "–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –≤—ñ–¥—Å—É—Ç–Ω—è", –Ω–∞–ø–∏—à–∏: "–ù–∞ –æ—Å–Ω–æ–≤—ñ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤ —è –∑–Ω–∞–π—à–æ–≤ –Ω–∞—Å—Ç—É–ø–Ω–µ..." —ñ –≤–∏–∫–ª–∞–¥–∏ –≤—Å–µ, —â–æ —Ö–æ—á —Ç—Ä–æ—Ö–∏ —Å—Ç–æ—Å—É—î—Ç—å—Å—è —Ç–µ–º–∏.
5. –ú–û–í–ê: –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –º–æ–≤–æ—é –∑–∞–ø–∏—Ç—É (—É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é).
"""

class LLMEngine:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(LLMEngine, cls).__new__(cls)
            cls._instance._model = None
        return cls._instance

    def load_model(self):
        if self._model is not None: return

        default_path = Path("/app/models_cache/qwen2.5-3b-instruct.gguf")
        if default_path.exists():
            model_path = default_path
        else:
            found = list(Path("/app/models_cache").glob("*.gguf"))
            if found: model_path = found[0]
            else: raise FileNotFoundError("Model not found!")

        logger.info(f"üöÄ Loading LLM: {model_path}")
        
        total_threads = os.cpu_count() or 4
        safe_threads = max(1, total_threads - 1)

        self._model = Llama(
            model_path=str(model_path),
            n_ctx=4096,           
            n_threads=safe_threads, 
            n_batch=512,
            use_mmap=True,        
            use_mlock=False,      
            verbose=False
        )

    def generate_stream(self, query: str, context: str):
        self.load_model()
        
        limit = 10000 
        if len(context) > limit:
            context = context[:limit] + "... [–∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä—ñ–∑–∞–Ω–æ]"

        messages = [
            {"role": "system", "content": SYSTEM_PROMPT_TEXT},
            {"role": "user", "content": f"–î–û–ö–£–ú–ï–ù–¢–ò:\n{context}\n\n–ó–ê–ü–ò–¢–ê–ù–ù–Ø –ö–û–†–ò–°–¢–£–í–ê–ß–ê:\n{query}"}
        ]

        stream = self._model.create_chat_completion(
            messages=messages,
            max_tokens=1024,
            temperature=0.3, 
            stream=True
        )

        for chunk in stream:
            delta = chunk["choices"][0]["delta"]
            if "content" in delta:
                yield delta["content"]
